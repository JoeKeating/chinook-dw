# ğŸµ Chinook Data Warehouse (chinook-dw)

Welcome to the Chinook Data Warehouse â€” a showcase data engineering project that models and transforms music industry data into a clean, dimensional warehouse using Snowflake, dbt, and Python.

## ğŸ“¦ Project Structure

```
chinook-dw-main/
â”œâ”€â”€ dbt/                  # dbt project: sources, models, macros
â”œâ”€â”€ src/                  # Python source files for data loading
â”œâ”€â”€ logs/                 # dbt logs
â”œâ”€â”€ .env.example          # Example environment configuration
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ Makefile              # Utility commands
â”œâ”€â”€ TODO.md               # Project roadmap
â””â”€â”€ README.md             # This file
```

## ğŸš€ Project Goals

- Build a foundational data warehouse using the [Chinook dataset](https://github.com/lerocha/chinook-database)
- Load and transform data into **landing**, **foundation**, and **core** zones
- Practice version-controlled, testable, and scalable data engineering techniques
- Prep for real-world use of **dbt**, **Snowflake**, and **SQLAlchemy**

## ğŸ› ï¸ Technologies Used

- **Snowflake** â€“ Cloud Data Platform
- **dbt Core** â€“ Transformation & Documentation
- **SQLAlchemy** â€“ Data loading & Python DB API
- **Python** â€“ ETL scripting
- **Makefile** â€“ Simplified command-line utility

## ğŸ§ª Getting Started

Before beginning, note that the database environment must be created manually by running SQL scripts located in src/sql/ddl/. These scripts must be executed in the following order:

1. create_db_and_schemas.sql

2. create_landing_tables.sql

3. create_users.sql

4. create_roles.sql

5. Clone the repo and create a virtual environment (compatible with both python3 -m venv and pyenv workflows):
   ```bash
   git clone https://github.com/yourusername/chinook-dw.git
cd chinook-dw
pyenv virtualenv 3.x.x chinook-dw  # or use python3 -m venv venv
pyenv local chinook-dw
pip install -r requirements.txt
   ```

6. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

7. Configure your AWS credentials to allow secure access to Snowflake connection secrets via AWS Secrets Manager.

8. Run initial data loads:
   ```bash
   python -m src.python.main load
   ```

9. Navigate into the `dbt` directory and run transformations:
   ```bash
   cd dbt
   dbt build
   ```

## ğŸ—‚ï¸ dbt Models

- **Sources**: Sources in landing zone defined in `models/sources/src_landing.yml`
- **Models**: To be built across  `foundation`, and `core`
- **Tests**: Place future assertions in `tests/`

## ğŸ¯ Version Roadmap

Track progress and planned enhancements in [`TODO.md`](TODO.md)


